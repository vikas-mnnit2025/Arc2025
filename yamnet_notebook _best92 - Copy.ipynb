{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af710b04-e459-4e90-8c07-aa5da7fb5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "import librosa\n",
    "import tensorflow.keras.backend as K\n",
    "import spec_augment\n",
    "import keras.backend as K\n",
    "import yamnet as yamnet_model\n",
    "import params as yamnet_params\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import yamnet as yamnet_model\n",
    "import params as yamnet_params\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from audiomentations import Compose, AddBackgroundNoise, TimeStretch, PitchShift, Shift, AddGaussianNoise\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbd3bb9f-247e-4333-b22a-d07d2c87737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetunable_yamnet():\n",
    "    params = yamnet_params.Params()\n",
    "    model = yamnet_model.yamnet_frames_model(params)\n",
    "    model.load_weights('yamnet.h5')  # download: https://storage.googleapis.com/audioset/yamnet.h5\n",
    "\n",
    "    for layer in model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b78565ab-9f75-4b6e-996f-cbf36419d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def augment_audio(audio, sr):\n",
    "#     if np.random.rand() < 0.5:\n",
    "#         audio = librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=np.random.randint(-2, 3))\n",
    "#     if np.random.rand() < 0.5:\n",
    "#         audio = librosa.effects.time_stretch(y=audio, rate=np.random.uniform(0.8, 1.2))\n",
    "#     if np.random.rand() < 0.5:\n",
    "#         noise = 0.005 * np.random.randn(len(audio))\n",
    "#         audio = audio + noise\n",
    "#     return np.clip(audio, -1.0, 1.0)\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain\n",
    "\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6, p=0.5)\n",
    "])\n",
    "\n",
    "# from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain, ClippingDistortion, PolarityInversion\n",
    "\n",
    "# augment = Compose([\n",
    "#     AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "#     TimeStretch(min_rate=0.85, max_rate=1.15, p=0.5),\n",
    "#     PitchShift(min_semitones=-3, max_semitones=3, p=0.5),\n",
    "#     Shift(min_shift=-0.2, max_shift=0.2, p=0.5),\n",
    "#     Gain(min_gain_db=-6, max_gain_db=6, p=0.5),\n",
    "#     ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.4),  # Random distortion\n",
    "#     PolarityInversion(p=0.3),  # Flip signal polarity\n",
    "# ])\n",
    "# Function to apply SpecAugment\n",
    "# def apply_specaugment(mel_spec):\n",
    "#     mel_spec = np.expand_dims(mel_spec, axis=0)  # Add batch dimension\n",
    "#     mel_spec = tf.convert_to_tensor(mel_spec, dtype=tf.float32)\n",
    "\n",
    "#     # Apply SpecAugment\n",
    "#     augmented_mel_spec = spec_augment.augment(mel_spec)\n",
    "\n",
    "#     return augmented_mel_spec.numpy().squeeze()  # Remove batch dimension\n",
    "\n",
    "def apply_specaugment(mel_spec, time_mask_param=10, freq_mask_param=8):\n",
    "    mel = mel_spec.copy()\n",
    "    num_mel_channels, num_time_steps = mel.shape\n",
    "\n",
    "    # Time mask\n",
    "    t = np.random.randint(0, time_mask_param)\n",
    "    t0 = np.random.randint(0, max(1, num_time_steps - t))\n",
    "    mel[:, t0:t0 + t] = 0\n",
    "\n",
    "    # Frequency mask\n",
    "    f = np.random.randint(0, freq_mask_param)\n",
    "    f0 = np.random.randint(0, max(1, num_mel_channels - f))\n",
    "    mel[f0:f0 + f, :] = 0\n",
    "\n",
    "    return mel\n",
    "\n",
    "\n",
    "def augment_audio(audio, sr):\n",
    "    audio = augment(samples=audio, sample_rate=sr)\n",
    "    return np.clip(audio, -1.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ec7184-e93f-4fcb-b9d8-efba53a562b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features_finetuned(yamnet, audio_path, sample_rate=16000):\n",
    "#     # try:\n",
    "#         y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "#         y = augment_audio(y, sr)  # <—— AUGMENTED HERE\n",
    "\n",
    "#         waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "#         waveform = tf.reshape(waveform, [-1])\n",
    "\n",
    "#         _, embeddings, _ = yamnet(waveform)\n",
    "#         avg_embedding = tf.reduce_mean(embeddings, axis=0).numpy()  # (1024,)\n",
    "\n",
    "#         mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=40)\n",
    "#         mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "#         mel_spec_flat = np.mean(mel_spec_db, axis=1)  # (40,)\n",
    "\n",
    "#         return np.concatenate([avg_embedding, mel_spec_flat])  # (1064,)\n",
    "def extract_features_finetuned(yamnet, audio_path, sample_rate=16000, use_specaugment=True):\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    y = augment_audio(y, sr)  # Apply other augmentations like time stretch, pitch shift\n",
    "\n",
    "    waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    waveform = tf.reshape(waveform, [-1])\n",
    "\n",
    "    # Get YAMNet embeddings\n",
    "    _, embeddings, _ = yamnet(waveform)\n",
    "    avg_embedding = tf.reduce_mean(embeddings, axis=0).numpy()  # (1024,)\n",
    "\n",
    "    # Create Mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=40)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Apply SpecAugment during training (not for validation/test)\n",
    "    if use_specaugment:\n",
    "        mel_spec_db = apply_specaugment(mel_spec_db)\n",
    "\n",
    "    mel_spec_flat = np.mean(mel_spec_db, axis=1)  # (40,)\n",
    "    \n",
    "    return np.concatenate([avg_embedding, mel_spec_flat])  # Concatenate YAMNet embeddings with Mel-spectrogram features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f00f1a-5c3e-48ba-841c-9f47b8c9a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_folder(base_path):\n",
    "    X, y = [], []\n",
    "    yamnet = load_finetunable_yamnet()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_labels = sorted(os.listdir(base_path))  # e.g., ['child_danger', 'normal', 'woman_danger']\n",
    "    label_encoder.fit(all_labels)\n",
    "\n",
    "    for label in tqdm(all_labels, desc=\"Processing folders\"):\n",
    "        folder_path = os.path.join(base_path, label)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                features = extract_features_finetuned(yamnet, file_path)\n",
    "                # features = extract_features_finetuned(yamnet, file_path, augment_audio=True)\n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = label_encoder.transform(y)\n",
    "    return X, y, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0effaca-acc4-4f44-81df-567f5584ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vishal Kumar Patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vishal Kumar Patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|                                                                        | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "X, y, label_encoder = extract_features_from_folder(r'C:\\Users\\Vishal Kumar Patel\\Desktop\\Yamnetmodelproject\\data\\train')\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Label shape:\", y.shape)\n",
    "print(\"Classes:\", label_encoder.classes_) \n",
    "np.save(\"X_embeddings_augmented.npy\", X)\n",
    "np.save(\"y_labels_augmented.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "070e0c83-fd57-4695-bf47-7efeada7c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load embeddings and labels from saved files\n",
    "X = np.load(\"X_embeddings_augmented.npy\")  # shape: (n_samples, 1024)\n",
    "y = np.load(\"y_labels_augmented.npy\")      # shape: (n_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37e6889f-d103-4902-82e7-1fcbdad37d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_19\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1064</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_20              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,221,632</span> │ input_layer_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_39        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ bidirectional_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ activation_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multiply_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
       "│                               │                           │                 │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ flatten_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_40        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1064\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_20              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m1,221,632\u001b[0m │ input_layer_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_39        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m1,024\u001b[0m │ bidirectional_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_39 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ batch_normalization_39[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m257\u001b[0m │ dropout_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ activation_20 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ dense_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multiply_20 (\u001b[38;5;33mMultiply\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ dropout_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
       "│                               │                           │                 │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ multiply_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten_19 (\u001b[38;5;33mFlatten\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ flatten_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_40        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │             \u001b[38;5;34m512\u001b[0m │ dense_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_40 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ batch_normalization_40[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m258\u001b[0m │ dropout_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,256,579</span> (4.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,256,579\u001b[0m (4.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,255,811</span> (4.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,255,811\u001b[0m (4.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Activation, Multiply, Flatten, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ----------------- Data Preprocessing -----------------\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_cat = to_categorical(y, num_classes)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Reshape for BiLSTM [samples, time_steps=1, features]\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=np.argmax(y_cat, axis=1))\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ----------------- Attention Block -----------------\n",
    "def attention_block(inputs):\n",
    "    attention_weights = Dense(1, activation='tanh')(inputs)\n",
    "    attention_weights = Activation('softmax')(attention_weights)\n",
    "    weighted_input = Multiply()([inputs, attention_weights])\n",
    "    context_vector = Lambda(lambda x: K.sum(x, axis=1))(weighted_input)\n",
    "    # context_vector = Lambda(lambda x: K.sum(x, axis=1), output_shape=(inputs.shape[1],))(weighted_input)  \n",
    "    return context_vector\n",
    "\n",
    "# ----------------- Model Definition -----------------\n",
    "input_layer = Input(shape=(1, X.shape[1]))  # Define input shape\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = attention_block(x)\n",
    "x = Flatten()(x) \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ----------------- Summary -----------------\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Dropout, Layer, Flatten, Multiply, Permute\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "# # One-hot encode labels\n",
    "# num_classes = len(np.unique(y))\n",
    "# y_cat = to_categorical(y, num_classes)\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# # Reshape for BiLSTM [samples, time_steps=1, features]\n",
    "# X_train = np.expand_dims(X_train, axis=1)\n",
    "# X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# # Compute class weights\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=np.argmax(y_cat, axis=1))\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# # ----------------- Attention Layer -----------------\n",
    "# class Attention(Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "#                                  initializer='normal', trainable=True)\n",
    "#         self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "#                                  initializer='zeros', trainable=True)\n",
    "#         super(Attention, self).build(input_shape)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "#         a = tf.keras.backend.softmax(e, axis=1)\n",
    "#         output = x * a\n",
    "#         return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# # ----------------- Model Architecture -----------------\n",
    "# input_layer = Input(shape=(1, 1064))  # Input shape\n",
    "# x = tf.keras.layers.Reshape((1064, 1))(input_layer)  # Reshape for LSTM\n",
    "\n",
    "# x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Attention()(x)\n",
    "\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce63f079-1999-40fb-9465-eee6b852d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishal Kumar Patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8081 - loss: 0.4968\n",
      "Epoch 1: val_loss improved from inf to 0.30803, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.8085 - loss: 0.4959 - val_accuracy: 0.8891 - val_loss: 0.3080 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8885 - loss: 0.2856\n",
      "Epoch 2: val_loss improved from 0.30803 to 0.28003, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.8885 - loss: 0.2855 - val_accuracy: 0.8830 - val_loss: 0.2800 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9051 - loss: 0.2503\n",
      "Epoch 3: val_loss improved from 0.28003 to 0.21336, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9051 - loss: 0.2502 - val_accuracy: 0.9185 - val_loss: 0.2134 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9101 - loss: 0.2307\n",
      "Epoch 4: val_loss improved from 0.21336 to 0.20268, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9101 - loss: 0.2307 - val_accuracy: 0.9224 - val_loss: 0.2027 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9126 - loss: 0.2272\n",
      "Epoch 5: val_loss did not improve from 0.20268\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9126 - loss: 0.2271 - val_accuracy: 0.9041 - val_loss: 0.2355 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9178 - loss: 0.2040\n",
      "Epoch 6: val_loss did not improve from 0.20268\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9178 - loss: 0.2040 - val_accuracy: 0.8570 - val_loss: 0.3572 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9234 - loss: 0.2029\n",
      "Epoch 7: val_loss did not improve from 0.20268\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9234 - loss: 0.2029 - val_accuracy: 0.9108 - val_loss: 0.2408 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9240 - loss: 0.1932\n",
      "Epoch 8: val_loss did not improve from 0.20268\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9240 - loss: 0.1932 - val_accuracy: 0.9152 - val_loss: 0.2182 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9309 - loss: 0.1806\n",
      "Epoch 9: val_loss did not improve from 0.20268\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9308 - loss: 0.1806 - val_accuracy: 0.9252 - val_loss: 0.2037 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9323 - loss: 0.1746\n",
      "Epoch 10: val_loss improved from 0.20268 to 0.18501, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9323 - loss: 0.1745 - val_accuracy: 0.9274 - val_loss: 0.1850 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9354 - loss: 0.1671\n",
      "Epoch 11: val_loss did not improve from 0.18501\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9354 - loss: 0.1671 - val_accuracy: 0.9224 - val_loss: 0.2092 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9282 - loss: 0.1746\n",
      "Epoch 12: val_loss did not improve from 0.18501\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9282 - loss: 0.1746 - val_accuracy: 0.9246 - val_loss: 0.1851 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9372 - loss: 0.1625\n",
      "Epoch 13: val_loss did not improve from 0.18501\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9372 - loss: 0.1625 - val_accuracy: 0.9257 - val_loss: 0.1853 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9410 - loss: 0.1512\n",
      "Epoch 14: val_loss improved from 0.18501 to 0.17899, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.9410 - loss: 0.1512 - val_accuracy: 0.9268 - val_loss: 0.1790 - learning_rate: 2.5000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9453 - loss: 0.1416\n",
      "Epoch 15: val_loss did not improve from 0.17899\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9453 - loss: 0.1416 - val_accuracy: 0.9235 - val_loss: 0.1962 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9452 - loss: 0.1395\n",
      "Epoch 16: val_loss did not improve from 0.17899\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9452 - loss: 0.1395 - val_accuracy: 0.9252 - val_loss: 0.1968 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9442 - loss: 0.1400\n",
      "Epoch 17: val_loss did not improve from 0.17899\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9442 - loss: 0.1399 - val_accuracy: 0.9235 - val_loss: 0.1911 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9468 - loss: 0.1366\n",
      "Epoch 18: val_loss did not improve from 0.17899\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.9468 - loss: 0.1365 - val_accuracy: 0.9274 - val_loss: 0.1813 - learning_rate: 1.2500e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9517 - loss: 0.1328\n",
      "Epoch 19: val_loss improved from 0.17899 to 0.17630, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9517 - loss: 0.1328 - val_accuracy: 0.9290 - val_loss: 0.1763 - learning_rate: 1.2500e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9487 - loss: 0.1318\n",
      "Epoch 20: val_loss improved from 0.17630 to 0.17607, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9487 - loss: 0.1318 - val_accuracy: 0.9296 - val_loss: 0.1761 - learning_rate: 1.2500e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9468 - loss: 0.1333\n",
      "Epoch 21: val_loss did not improve from 0.17607\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9469 - loss: 0.1333 - val_accuracy: 0.9302 - val_loss: 0.1838 - learning_rate: 1.2500e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9499 - loss: 0.1272\n",
      "Epoch 22: val_loss did not improve from 0.17607\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9499 - loss: 0.1272 - val_accuracy: 0.9307 - val_loss: 0.1765 - learning_rate: 1.2500e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9531 - loss: 0.1258\n",
      "Epoch 23: val_loss did not improve from 0.17607\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9531 - loss: 0.1258 - val_accuracy: 0.9257 - val_loss: 0.1870 - learning_rate: 1.2500e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9523 - loss: 0.1234\n",
      "Epoch 24: val_loss improved from 0.17607 to 0.17539, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9523 - loss: 0.1234 - val_accuracy: 0.9302 - val_loss: 0.1754 - learning_rate: 6.2500e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9498 - loss: 0.1209\n",
      "Epoch 25: val_loss improved from 0.17539 to 0.17489, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.9498 - loss: 0.1209 - val_accuracy: 0.9296 - val_loss: 0.1749 - learning_rate: 6.2500e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9535 - loss: 0.1209\n",
      "Epoch 26: val_loss did not improve from 0.17489\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9535 - loss: 0.1209 - val_accuracy: 0.9263 - val_loss: 0.1750 - learning_rate: 6.2500e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9553 - loss: 0.1129\n",
      "Epoch 27: val_loss did not improve from 0.17489\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.9553 - loss: 0.1128 - val_accuracy: 0.9268 - val_loss: 0.1765 - learning_rate: 6.2500e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9532 - loss: 0.1154\n",
      "Epoch 28: val_loss did not improve from 0.17489\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9532 - loss: 0.1154 - val_accuracy: 0.9302 - val_loss: 0.1789 - learning_rate: 6.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9563 - loss: 0.1115\n",
      "Epoch 29: val_loss did not improve from 0.17489\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9563 - loss: 0.1114 - val_accuracy: 0.9307 - val_loss: 0.1750 - learning_rate: 3.1250e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9573 - loss: 0.1153\n",
      "Epoch 30: val_loss improved from 0.17489 to 0.17451, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9573 - loss: 0.1153 - val_accuracy: 0.9329 - val_loss: 0.1745 - learning_rate: 3.1250e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9568 - loss: 0.1084\n",
      "Epoch 31: val_loss did not improve from 0.17451\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9568 - loss: 0.1084 - val_accuracy: 0.9290 - val_loss: 0.1763 - learning_rate: 3.1250e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9563 - loss: 0.1082\n",
      "Epoch 32: val_loss improved from 0.17451 to 0.17401, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9563 - loss: 0.1082 - val_accuracy: 0.9313 - val_loss: 0.1740 - learning_rate: 3.1250e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9583 - loss: 0.1092\n",
      "Epoch 33: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9583 - loss: 0.1092 - val_accuracy: 0.9324 - val_loss: 0.1761 - learning_rate: 3.1250e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9592 - loss: 0.1087\n",
      "Epoch 34: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9592 - loss: 0.1087 - val_accuracy: 0.9302 - val_loss: 0.1756 - learning_rate: 3.1250e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9618 - loss: 0.1029\n",
      "Epoch 35: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9618 - loss: 0.1029 - val_accuracy: 0.9313 - val_loss: 0.1761 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9581 - loss: 0.1061\n",
      "Epoch 36: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9581 - loss: 0.1061 - val_accuracy: 0.9302 - val_loss: 0.1762 - learning_rate: 1.5625e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9576 - loss: 0.1025\n",
      "Epoch 37: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9576 - loss: 0.1025 - val_accuracy: 0.9318 - val_loss: 0.1772 - learning_rate: 1.5625e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9624 - loss: 0.1037\n",
      "Epoch 38: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9623 - loss: 0.1037 - val_accuracy: 0.9307 - val_loss: 0.1766 - learning_rate: 1.5625e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9593 - loss: 0.1076\n",
      "Epoch 39: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9593 - loss: 0.1076 - val_accuracy: 0.9290 - val_loss: 0.1768 - learning_rate: 7.8125e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9579 - loss: 0.1074\n",
      "Epoch 40: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9579 - loss: 0.1073 - val_accuracy: 0.9307 - val_loss: 0.1776 - learning_rate: 7.8125e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9552 - loss: 0.1087\n",
      "Epoch 41: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9553 - loss: 0.1087 - val_accuracy: 0.9296 - val_loss: 0.1784 - learning_rate: 7.8125e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m225/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9607 - loss: 0.1052\n",
      "Epoch 42: val_loss did not improve from 0.17401\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.9607 - loss: 0.1052 - val_accuracy: 0.9290 - val_loss: 0.1777 - learning_rate: 3.9063e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29f3dbfcd40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=50,\n",
    "#     batch_size=64,\n",
    "#     class_weight=class_weights_dict,\n",
    "#     callbacks=[early_stop],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=50,\n",
    "#     batch_size=64,\n",
    "#     class_weight=class_weights_dict,\n",
    "#     callbacks=[reduce_lr, early_stop],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ----------------- Callbacks -----------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# ----------------- Train Model -----------------\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e736e1b-48d1-4b31-bb13-6fc69cbc6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9370 - loss: 0.1684\n",
      "✅ Test Accuracy: 93.13%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"✅ Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c9a9ec4-395d-4d18-86dc-74575e71a23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/226\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 361ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishal Kumar Patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       904\n",
      "           1       0.93      0.93      0.93       900\n",
      "\n",
      "    accuracy                           0.93      1804\n",
      "   macro avg       0.93      0.93      0.93      1804\n",
      "weighted avg       0.93      0.93      0.93      1804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predicted_test = model.predict(X_test)\n",
    "# predicted_train = model.predict(X_train)\n",
    "# predicted_class = []\n",
    "# for i in range(len(predicted_train)):\n",
    "#   predicted_class.append(np.argmax(predicted_train[i]))\n",
    "# predicted_class_index = []\n",
    "# for i in range(len(predicted_test)):\n",
    "#   predicted_class_index.append(np.argmax(predicted_test[i]))\n",
    "# rounded_test = np.argmax(y_test,axis=1)\n",
    "# rounded_train = np.argmax(y_train,axis=1)\n",
    "\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(rounded_train, predicted_class)\n",
    "\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(rounded_test, predicted_class_index)\n",
    "\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# print(classification_report(rounded_test, predicted_class_index))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict on train and test sets\n",
    "predicted_train = model.predict(X_train)\n",
    "predicted_test = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to class indices\n",
    "predicted_train_classes = np.argmax(predicted_train, axis=1)\n",
    "predicted_test_classes = np.argmax(predicted_test, axis=1)\n",
    "\n",
    "# Convert one-hot encoded true labels to class indices\n",
    "true_train_classes = np.argmax(y_train, axis=1)\n",
    "true_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# ----------------- Train Confusion Matrix -----------------\n",
    "cm_train = confusion_matrix(true_train_classes, predicted_train_classes)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train)\n",
    "disp_train.plot()\n",
    "plt.title(\"Train Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------- Test Confusion Matrix -----------------\n",
    "cm_test = confusion_matrix(true_test_classes, predicted_test_classes)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test)\n",
    "disp_test.plot()\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------- Classification Report -----------------\n",
    "print(\"Classification Report (Test Data):\")\n",
    "print(classification_report(true_test_classes, predicted_test_classes))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b05875e2-3dc2-4583-902c-b6677b6f4567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "woman_danger       0.93      0.93      0.93       904\n",
      "      normal       0.93      0.93      0.93       900\n",
      "\n",
      "    accuracy                           0.93      1804\n",
      "   macro avg       0.93      0.93      0.93      1804\n",
      "weighted avg       0.93      0.93      0.93      1804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=['woman_danger', 'normal']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83c058e8-4452-4c06-bc5f-ac2a1abdce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('final_best.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aced2d9a-dd91-4af6-92b3-821613e78874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with the same architecture...\n",
      "Loading weights from saved model...\n",
      "✅ Model weights loaded successfully!\n",
      "\n",
      "Classifying audio file: C:\\Users\\Vishal Kumar Patel\\Desktop\\Yamnetmodelproject\\data\\test\\Normal\\112_3_right.wav\n",
      "Class: Normal, Probability: 0.0029\n",
      "Class: Danger, Probability: 0.9971\n",
      "\n",
      "The audio is classified as: Danger\n",
      "Confidence: 0.9971\n",
      "\n",
      "FINAL RESULT:\n",
      "Predicted: Danger with 99.71% confidence.\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import librosa\n",
    "# import random\n",
    "# import tensorflow_hub as hub\n",
    "# from tensorflow.keras.models import load_model, Model\n",
    "# from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Activation, Multiply, Flatten, Lambda\n",
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "# # ----------------- Set seeds for reproducibility -----------------\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "# random.seed(42)\n",
    "\n",
    "# # ----------------- Custom Layers & Functions -----------------\n",
    "# class SumLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(SumLayer, self).__init__(**kwargs)\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         return K.sum(inputs, axis=1)\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (input_shape[0], input_shape[2])\n",
    "\n",
    "# def normalize_input(x):\n",
    "#     return (x / tf.reduce_max(x)) - tf.reduce_mean(x)\n",
    "\n",
    "# def attention_mechanism(inputs):\n",
    "#     attention_weights = Dense(1, activation='tanh')(inputs)\n",
    "#     attention_weights = Activation('softmax')(attention_weights)\n",
    "#     weighted_input = Multiply()([inputs, attention_weights])\n",
    "#     return SumLayer()(weighted_input)\n",
    "\n",
    "# # Define custom lambda layer with explicit output shape\n",
    "# def sum_layer_wrapper(x):\n",
    "#     return K.sum(x, axis=1)\n",
    "\n",
    "# def sum_layer_shape(input_shape):\n",
    "#     return (input_shape[0], input_shape[2])\n",
    "\n",
    "# def extract_yamnet_embedding(audio_path):\n",
    "#     yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "#     waveform, sr = librosa.load(audio_path, sr=16000)\n",
    "#     waveform = waveform[:sr * 10]  # Limit to 10 seconds\n",
    "#     waveform = tf.convert_to_tensor(waveform, dtype=tf.float32)\n",
    "#     _, embeddings, _ = yamnet_model(waveform)\n",
    "#     embedding_mean = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "    \n",
    "#     if embedding_mean.shape[0] < 1064:\n",
    "#         embedding_mean = np.pad(embedding_mean, (0, 1064 - embedding_mean.shape[0]), mode='constant')\n",
    "#     else:\n",
    "#         embedding_mean = embedding_mean[:1064]\n",
    "        \n",
    "#     return embedding_mean\n",
    "\n",
    "# def test_audio(file_path, model, classes):\n",
    "#     yamnet_embedding = extract_yamnet_embedding(file_path)\n",
    "#     input_data = np.expand_dims(np.expand_dims(yamnet_embedding, axis=0), axis=1)\n",
    "    \n",
    "#     try:\n",
    "#         predictions = model.predict(input_data, verbose=0)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during prediction: {e}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     class_probabilities = predictions[0]\n",
    "#     predicted_class_index = np.argmax(class_probabilities)\n",
    "    \n",
    "#     return class_probabilities, predicted_class_index\n",
    "\n",
    "# # ----------------- Main Execution -----------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     classes = ['Normal', 'Danger']  # Class order\n",
    "\n",
    "#     # Define custom objects you used\n",
    "#     custom_objects = {\n",
    "#         'normalize_input': normalize_input,\n",
    "#         'SumLayer': SumLayer,\n",
    "#         'attention_mechanism': attention_mechanism,\n",
    "#         '<lambda>': Lambda(sum_layer_wrapper, output_shape=sum_layer_shape)\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         # First try loading with compile=False\n",
    "#         print(\"Attempting to load model...\")\n",
    "#         model = tf.keras.models.load_model('final_best.h5', \n",
    "#                                           custom_objects=custom_objects, \n",
    "#                                           compile=False)\n",
    "#         print(\"✅ Model loaded successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading model: {e}\")\n",
    "#         try:\n",
    "#             # Try alternative loading method\n",
    "#             print(\"Trying alternative loading method...\")\n",
    "#             model = tf.keras.models.load_model('final_best.h5', \n",
    "#                                               custom_objects=custom_objects, \n",
    "#                                               compile=False,\n",
    "#                                               options=tf.saved_model.LoadOptions(experimental_io_device='/job:localhost'))\n",
    "#             print(\"✅ Model loaded successfully with alternative method!\")\n",
    "#         except Exception as e2:\n",
    "#             print(f\"Both loading methods failed: {e2}\")\n",
    "#             print(\"Please make sure the model file exists and is not corrupted.\")\n",
    "#             exit(1)\n",
    "\n",
    "#     model.trainable = False\n",
    "#     # r'C:\\Users\\Vishal Kumar Patel\\Desktop\\Yamnetmodelproject\\data\\test\\Women\\121_11_patch.wav'\n",
    "#     test_audio_file = r'C:\\Users\\Vishal Kumar Patel\\Desktop\\Yamnetmodelproject\\data\\test\\Normal\\119_6_right.wav'\n",
    "#     print(f\"\\nClassifying audio file: {test_audio_file}\")\n",
    "    \n",
    "#     class_probabilities, predicted_class_index = test_audio(test_audio_file, model, classes)\n",
    "    \n",
    "#     if class_probabilities is not None and predicted_class_index is not None:\n",
    "#         for i, class_label in enumerate(classes):\n",
    "#             print(f'Class: {class_label}, Probability: {class_probabilities[i]:.4f}')\n",
    "#         print(f'\\nThe audio is classified as: {classes[predicted_class_index]}')\n",
    "#         print(f'Confidence: {class_probabilities[predicted_class_index]:.4f}')\n",
    "        \n",
    "#         # Final result\n",
    "#         print(\"\\nFINAL RESULT:\")\n",
    "#         print(f\"Predicted: {classes[predicted_class_index]} with {class_probabilities[predicted_class_index]:.2%} confidence.\")\n",
    "#     else:\n",
    "#         print(\"Prediction failed, no results to display.\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import random\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Activation, Multiply, Flatten, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ----------------- Set seeds for reproducibility -----------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ----------------- Custom Layers & Functions -----------------\n",
    "class SumLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SumLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return K.sum(inputs, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])\n",
    "\n",
    "def normalize_input(x):\n",
    "    return (x / tf.reduce_max(x)) - tf.reduce_mean(x)\n",
    "\n",
    "# Fixed attention block with proper Lambda layer output shape\n",
    "def attention_block(inputs):\n",
    "    attention_weights = Dense(1, activation='tanh')(inputs)\n",
    "    attention_weights = Activation('softmax')(attention_weights)\n",
    "    weighted_input = Multiply()([inputs, attention_weights])\n",
    "    # Define proper output shape for Lambda layer\n",
    "    context_vector = Lambda(lambda x: K.sum(x, axis=1), \n",
    "                           output_shape=lambda input_shape: (input_shape[0], input_shape[2]))(weighted_input)\n",
    "    return context_vector\n",
    "\n",
    "def extract_yamnet_embedding(audio_path):\n",
    "    yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "    waveform, sr = librosa.load(audio_path, sr=16000)\n",
    "    waveform = waveform[:sr * 10]  # Limit to 10 seconds\n",
    "    waveform = tf.convert_to_tensor(waveform, dtype=tf.float32)\n",
    "    _, embeddings, _ = yamnet_model(waveform)\n",
    "    embedding_mean = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "    \n",
    "    if embedding_mean.shape[0] < 1064:\n",
    "        embedding_mean = np.pad(embedding_mean, (0, 1064 - embedding_mean.shape[0]), mode='constant')\n",
    "    else:\n",
    "        embedding_mean = embedding_mean[:1064]\n",
    "        \n",
    "    return embedding_mean\n",
    "\n",
    "def test_audio(file_path, model, classes):\n",
    "    yamnet_embedding = extract_yamnet_embedding(file_path)\n",
    "    input_data = np.expand_dims(np.expand_dims(yamnet_embedding, axis=0), axis=1)\n",
    "    \n",
    "    try:\n",
    "        predictions = model.predict(input_data, verbose=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    class_probabilities = predictions[0]\n",
    "    predicted_class_index = np.argmax(class_probabilities)\n",
    "    \n",
    "    return class_probabilities, predicted_class_index\n",
    "\n",
    "# Recreate the model with the same architecture as your original model\n",
    "def create_model(input_shape=(1, 1064), num_classes=2):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Using the fixed attention block\n",
    "    x = attention_block(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ----------------- Main Execution -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    classes = ['Normal', 'Danger']  # Class order\n",
    "\n",
    "    # Create a new model with the same architecture\n",
    "    print(\"Creating model with the same architecture...\")\n",
    "    model = create_model(input_shape=(1, 1064), num_classes=2)\n",
    "    \n",
    "    # Load weights from the saved model\n",
    "    try:\n",
    "        print(\"Loading weights from saved model...\")\n",
    "        model.load_weights('final_best.h5')\n",
    "        print(\"✅ Model weights loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights: {e}\")\n",
    "        print(\"Using untrained model - results will be random.\")\n",
    "    \n",
    "    model.trainable = False\n",
    "    \n",
    "    test_audio_file = r'C:\\Users\\Vishal Kumar Patel\\Desktop\\Yamnetmodelproject\\data\\test\\Normal\\112_3_right.wav'\n",
    "    print(f\"\\nClassifying audio file: {test_audio_file}\")\n",
    "    \n",
    "    class_probabilities, predicted_class_index = test_audio(test_audio_file, model, classes)\n",
    "    \n",
    "    if class_probabilities is not None and predicted_class_index is not None:\n",
    "        for i, class_label in enumerate(classes):\n",
    "            print(f'Class: {class_label}, Probability: {class_probabilities[i]:.4f}')\n",
    "        print(f'\\nThe audio is classified as: {classes[predicted_class_index]}')\n",
    "        print(f'Confidence: {class_probabilities[predicted_class_index]:.4f}')\n",
    "        \n",
    "        # Final result\n",
    "        print(\"\\nFINAL RESULT:\")\n",
    "        print(f\"Predicted: {classes[predicted_class_index]} with {class_probabilities[predicted_class_index]:.2%} confidence.\")\n",
    "    else:\n",
    "        print(\"Prediction failed, no results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa688f-ac81-4163-8495-8da88eee5f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
